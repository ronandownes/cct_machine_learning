{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1\n",
    "## Machine Learning for Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filename=r'iris.png': The r in front of the string makes it a raw string literal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename ='iris.png', width = 400, height = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(Image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ssued\r\n",
    "\r\n",
    "xceptions\r\n",
    "hing warnidless of gs, regardless of location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('once')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(warnings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Iris Dataset\r\n",
    "\r\n",
    "The `sklearn.datasets` module provides several datasets that are commonly used in machine learning, including the famous Iris dataset. The Iris dataset contains measurements of various features (such as petal length and sepal width) for three different species of iris flowers. Each sample in the dataset is labeled with the species it belongs to.\r\n",
    "\r\n",
    "In this example, we'll load the Iris dataset and extract the data (features) from it. The `load_iris()` function returns a Bunch object, which is similar to a dictionary, containing all the data and other information like the feature names and target labels.\r\n",
    "\r\n",
    "Let's start by loading the dataset and extracting the feature data.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the load_iris function from sklearn.datasets module to load the Iris dataset\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable               Type        Data/Info\n",
      "--------------------------------------------\n",
      "dataframe_columns      function    <function dataframe_colum<...>ns at 0x000001F04803F920>\n",
      "dataframe_hash         function    <function dataframe_hash at 0x000001F04803F420>\n",
      "dtypes_str             function    <function dtypes_str at 0x000001F04803C7C0>\n",
      "get_dataframes         function    <function get_dataframes at 0x000001F049683D80>\n",
      "getpass                module      <module 'getpass' from 'C<...>conda3\\\\Lib\\\\getpass.py'>\n",
      "hashlib                module      <module 'hashlib' from 'C<...>conda3\\\\Lib\\\\hashlib.py'>\n",
      "import_pandas_safely   function    <function import_pandas_s<...>ly at 0x000001F04B77D120>\n",
      "is_data_frame          function    <function is_data_frame at 0x000001F04803FCE0>\n",
      "json                   module      <module 'json' from 'C:\\\\<...>\\Lib\\\\json\\\\__init__.py'>\n",
      "load_iris              function    <function load_iris at 0x000001F04B61C400>\n"
     ]
    }
   ],
   "source": [
    "%whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.dataframe_columns(var)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
       "0                5.1               3.5                1.4               0.2\n",
       "1                4.9               3.0                1.4               0.2\n",
       "2                4.7               3.2                1.3               0.2\n",
       "3                4.6               3.1                1.5               0.2\n",
       "4                5.0               3.6                1.4               0.2"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load the Iris dataset and return it as a pandas DataFrame\n",
    "iris_df = load_iris(as_frame=True)\n",
    "\n",
    "# Access the DataFrame containing the feature data\n",
    "iris_df = iris_df.data\n",
    "iris_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, understanding the data structures you're working with—such as NumPy arrays, pandas DataFrames, and the Bunch object in scikit-learn—is essential knowledge in data science and machine learning. These are foundational concepts that will help you work effectively with data and build machine learning models. Here’s why this knowledge is crucial:\r\n",
    "\r\n",
    "### 1. **NumPy Arrays**\r\n",
    "   - **Efficiency**: NumPy arrays are the backbone of numerical computing in Python. They are highly efficient for performing mathematical and statistical operations on large datasets.\r\n",
    "   - **Operations**: Understanding how to manipulate and operate on arrays (like slicing, reshaping, and broadcasting) is key to processing and analyzing data.\r\n",
    "\r\n",
    "### 2. **pandas DataFrames**\r\n",
    "   - **Data Manipulation**: pandas DataFrames provide a flexible way to store and manipulate tabular data. They support labeled rows and columns, making it easier to work with data that has labels or identifiers (like feature names and target labels).\r\n",
    "   - **Ease of Use**: DataFrames come with a rich set of functions for filtering, aggregating, and transforming data, which is invaluable when preparing data for analysis or modeling.\r\n",
    "\r\n",
    "### 3. **Bunch Object in scikit-learn**\r\n",
    "   - **Convenience**: The Bunch object is a simple but powerful way to package different parts of a dataset together (like data, target, feature names) in a dictionary-like structure. This allows easy access to all parts of the dataset without needing to load them separately.\r\n",
    "   - **Organization**: It helps in keeping your data well-organized, especially when dealing with datasets that include multiple components (features, labels, descriptions, etc.).\r\n",
    "\r\n",
    "### Why Is This Essential Knowledge?\r\n",
    "\r\n",
    "- **Data Preprocessing**: Most machine learning projects start with data cleaning and preprocessing, which often involves manipulating arrays or DataFrames. Knowing how to efficiently handle these structures can save a lot of time and prevent errors.\r\n",
    "  \r\n",
    "- **Model Building**: When building models, you’ll frequently convert between different data structures. For example, you might train a model on NumPy arrays but evaluate it on a pandas DataFrame. Understanding how these structures work ensures that you can do this seamlessly.\r\n",
    "\r\n",
    "- **Debugging**: If you encounter errors or unexpected results, a strong understanding of these data structures can help you diagnose and fix issues more effectively.\r\n",
    "\r\n",
    "### Summary\r\n",
    "\r\n",
    "Yes, this is essential data structure knowledge. Mastering these concepts will empower you to handle data more efficiently and effectively, which is critical in data science, machine learning, and any data-driven field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the NumPy library as np, a common alias in the Python community\n",
    "import numpy as np\n",
    "\n",
    "# Load the Iris dataset and extract the feature data\n",
    "# The .data attribute contains the features of the dataset, stored as a NumPy array\n",
    "iris = load_iris()  # Load the entire Iris dataset into a variable named 'iris'\n",
    "data = iris.data    # Extract only the feature data from the dataset\n",
    "\n",
    "# You can now use NumPy to perform operations on the data\n",
    "# For example, let's calculate the mean of each feature (column) across all samples (rows)\n",
    "mean_features = np.mean(data, axis=0)\n",
    "\n",
    "# Now, 'mean_features' contains the average value for each feature (like average petal length, etc.)\n",
    "# This is useful for understanding the overall distribution of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's in the `data` Variable?\r\n",
    "\r\n",
    "The `data` variable now contains a NumPy array where:\r\n",
    "- Each row corresponds to a different flower sample (a total of 150 samples).\r\n",
    "- Each column corresponds to a feature (like sepal length, sepal width, petal length, petal width).\r\n",
    "\r\n",
    "This data will be used in later steps to train and evaluate machine learning models.\r\n",
    "\r\n",
    "Next, we can explore this data further or move on to using it in a machine learning model.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the load_iris function from sklearn.datasets module to load the Iris dataset\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Import the NumPy library as np, a common alias in the Python community\n",
    "import numpy as np\n",
    "\n",
    "# Load the Iris dataset and extract the feature data\n",
    "# The .data attribute contains the features of the dataset, stored as a NumPy array\n",
    "iris = load_iris()  # Load the entire Iris dataset into a variable named 'iris'\n",
    "data = iris.data    # Extract only the feature data from the dataset\n",
    "\n",
    "# You can now use NumPy to perform operations on the data\n",
    "# For example, let's calculate the mean of each feature (column) across all samples (rows)\n",
    "mean_features = np.median(data, axis=0)\n",
    "median_features = np.median(data, axis=0)\n",
    "max_features = np.max(data, axis=0)\n",
    "min_features = np.min(data, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "# Now, 'mean_features' contains the average value for each feature (like average petal length, etc.)\n",
    "# This is useful for understanding the overall distribution of the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data variable will be a numpy array of shape (150,4) having 150 samples each having four different attributes. Each class has 50 samples each.\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Data Type: NumPy Array vs. DataFrame\r\n",
    "\r\n",
    "When we load the Iris dataset using `load_iris()` from `sklearn.datasets`, the feature data is returned as a NumPy array, not a pandas DataFrame. \r\n",
    "\r\n",
    "- **NumPy Array**: A NumPy array is a powerful data structure that allows for efficient numerical operations. It's particularly useful for mathematical computations and manipulating numerical data.\r\n",
    "- **DataFrame**: A pandas DataFrame is another type of data structure that is more flexible and user-friendly for data manipulation, particularly when working with labeled data (i.e., data with column names).\r\n",
    "\r\n",
    "In this example, we are working with a NumPy array, which means that we don't have the column names directly attached to the data. Instead, the array is purely numerical, and we would refer to the columns by their indices.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# Extract the target labels\n",
    "target = iris.target\n",
    "\n",
    "# Print the target labels\n",
    "print(target)\n",
    "(target)\r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_iris().target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the class labels.\n",
    "labels = load_iris().target\n",
    "labels\n",
    "# First 50 samples belongs to class 0, next 50 samples belong to class 1 and third class as 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, you have to # Combine the data and the class labels, and for that,we use an excellent python library called NumPy. \n",
    "# NumPy adds support for large, multi-dimensional arrays and matrices\n",
    "import numpy as np\n",
    "# Since data is a 2-d array, we have to reshape the labels also to a 2-d array.\n",
    "labels = np.reshape(labels,(150, 1))\n",
    "# We use the concatenate function available in the numpy library, and use axis=-1 which will concatenate based on the 2nd dimension.\n",
    "\n",
    "data = np.concatenate([data,labels], axis = -1)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import python's data analysis library called pandas which is useful when you want to arrange your data in a tabular fashion \n",
    "# and perform some operations and manipulations on the data.\n",
    "import pandas as pd\n",
    "names = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'species']   # Store the column headings\n",
    "dataset = pd.DataFrame(data,columns=names)                                          # Create a dataframe based on data array and column names\n",
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have the dataset data frame that has both data & the class labels\n",
    "# The labels variable has class labels as numeric values, but you will convert the numeric values as the flower names or species.\n",
    "# We select only the class column and replace each of the three numeric values with the corresponding species. \n",
    "# We update the labels based on the three species (Iris-setosa as 0, Iris-versicolor as 1, Iris-virginica as 2)\n",
    "dataset['species'].replace(0, 'Iris-setosa', inplace = True)\n",
    "dataset['species'].replace(1, 'Iris-versicolor', inplace = True)\n",
    "dataset['species'].replace(2, 'Iris-virginica', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first five rows of the dataset and see what it looks like!\n",
    "\n",
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze iris data\n",
    "We find out how all the three flowers look like when visualized and how different they are from each other.\n",
    "Further details of this dataset are available on https://www.kaggle.com/arshid/iris-flower-dataset.\n",
    "Let's visualize the data that you loaded above using a scatterplot to find out how much one variable is affected by the other variable or let's say how much correlation is between the two variables. We use matplotlib library to visualize the data using a scatterplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt      # import the library matplotlib and stored as an object plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(4, figsize=(10, 8))                                               # Size of the figure as length and width\n",
    "# data[:50,0] means first 50 rows for Sepal length (0), and 0 means Sepal length\n",
    "# data[:50,1] means first 50 rows for Sepal width (1), and 1 means Sepal width\n",
    "plt.scatter(data[:50, 0], data[:50, 1], c='r', label='Iris-setosa')          # Taking only first 50 rows for Iris-setosa\n",
    "\n",
    "plt.scatter(data[50:100, 0], data[50:100, 1], c='g',label='Iris-versicolor')  # Taking next 50 rows for Iris-versicolor\n",
    "\n",
    "plt.scatter(data[100:, 0], data[100:, 1], c='b',label='Iris-virginica')       # Taking next 50 rows for Iris-virginica\n",
    "\n",
    "plt.xlabel('Sepal length', fontsize = 20)\n",
    "plt.ylabel('Sepal width', fontsize = 20)\n",
    "plt.xticks(fontsize = 20)\n",
    "plt.yticks(fontsize = 20)\n",
    "plt.title('Sepal length vs. Sepal width',fontsize = 20)\n",
    "plt.legend(prop = {'size': 18})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above plot, it is very much apparent that there is a high correlation between the Iris setosa flowers w.r.t the sepal length and sepal width. On the other hand, there is less correlation between Iris versicolor and Iris virginica. The data points in versicolor & virginica are more spread out compared to setosa that are dense.\n",
    "\n",
    "Now we plot the graph for petal-length and petal-width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(4, figsize=(8, 8))\n",
    "# data[:50,2] means first 50 rows for Petal length (2), and 2 means Petal length\n",
    "# data[:50,3] means first 50 rows for Petal width (3), and 3 means Petal width\n",
    "plt.scatter(data[:50, 2], data[:50, 3], c='r', label='Iris-setosa')\n",
    "\n",
    "plt.scatter(data[50:100, 2], data[50:100, 3], c='g',label='Iris-versicolor')\n",
    "\n",
    "plt.scatter(data[100:, 2], data[100:, 3], c='b',label='Iris-virginica')\n",
    "plt.xlabel('Petal length',fontsize = 15)\n",
    "plt.ylabel('Petal width',fontsize = 15)\n",
    "plt.xticks(fontsize = 15)\n",
    "plt.yticks(fontsize = 15)\n",
    "plt.title('Petal length vs. Petal width',fontsize = 15)\n",
    "plt.legend(prop={'size': 20})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that the petal-length and petal-width indicate a strong correlation for setosa flowers which are densely clustered together.\n",
    "To further validate the claim of how petal-length and petal-width are correlated, let's plot a correlation matrix for all the three species."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heatmap:\n",
    "Heatmaps visualize the data in a 2-dimensional format in the form of colored maps. The color maps use hue, saturation, or luminance to achieve color variation to display various details. This color variation gives visual description to the readers about the magnitude of numeric values. HeatMaps is about replacing numbers with colors because the human brain understands visuals better than numbers, text, or any written data. Human beings are visual learners; therefore, visualizing the data in any form makes more sense.\n",
    "Heatmaps can describe the density or intensity of variables, visualize patterns, variance, and even anomalies. Heatmaps show relationships between variables. These variables are plotted on both axes. We look for patterns in the cell by noticing the color change. It only accepts numeric data and plots it on the grid, displaying different data values by varying color intensity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.heatmap(dataset.corr(), annot = True, fmt = '.2f', linewidths = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the number of records per class\n",
    "print(dataset.groupby('species').size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing iris data\n",
    "After having loaded the data and analyzed it extensively, it is time to prepare the data which we can then feed to our ML model. We can preprocess the data in two ways: \n",
    "(i)  Normalizing your data and \n",
    "(ii) Splitting your data into training and testing sets\n",
    "\n",
    "### Normalizing your data\n",
    "There can be two ways by which you can normalize your data. Now the question is why or when do you need to normalize your data? And do you need to standardize the Iris data?\n",
    "\n",
    "It is a good practice to normalize your data as it brings all the samples in the same scale and range. Normalizing the data is crucial when the data you have is not consistent. We can check for inconsistency by using the describe() function that you studied above which will give usmax and min values. If the max and min values of one feature are significantly larger than the other feature then normalizing both the features to the same scale is very important.\n",
    "\n",
    "Let's say X is one feature having a larger range and Y being the second feature with a smaller range. Then, the influence of feature Y can be overpowered by feature X's influence. In such a case, it becomes important to normalize both the features X and Y.\n",
    "\n",
    "In Iris data, normalization is not required. Let's print the describe() function again and see why you do not need any normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sepal-length attribute has values that go from 4.3 to 7.9 and sepal-width contains values from 2 to 4.4, while petal-length values range from 1 to 6.9 and petal-width ranges from 0.1 to 2.5. The values of all the features are within the range of 0.1 and 7.9, which can consider as acceptable. Hence, we do not need to apply any normalization to the Iris dataset.\n",
    "\n",
    "## Splitting the data\n",
    "This is another significant aspect of machine learning since your goal is to make a model capable enough to be able to take decisions or classify data in a test environment without any human intervention. Hence, before deploying your ML model in the industry, we need to make sure that the model can generalize well on the testing data.\n",
    "\n",
    "For this purpose, we need a training and testing set. Coming back to the Iris data, we have 150 samples, we will be training your ML model on 80% of the data and the remaining 20% of the data will be used for testing.\n",
    "\n",
    "In data-science, we come across a term called Overfitting which means that your model has learned the training data very well but fails to perform on the testing data. So, splitting the data into training and testing or validation set help uso know whether ourodel is overfitting or not.\n",
    "\n",
    "For training and testing set split, we use the sklearn library which has an in-built splitting function called train_test_split. So, let's split the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_data,test_data,train_label,test_label = train_test_split(dataset.iloc[:,:3], \n",
    "                                                                dataset.iloc[:,4], test_size=0.2, random_state=42)\n",
    "# We split the data for Petal length (iloc[:,:3]) and Petal width (iloc[:,:4]), 0.2 means 20% for testing and random_state \n",
    "# simply sets a seed to the random generator, so that your train-test splits are always deterministic. \n",
    "# If we don't set a seed, it is different each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shape of training and testing data along with its labels.\n",
    "\n",
    "train_data.shape,train_label.shape,test_data.shape,test_label.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The KNN Model\n",
    "After loading, analyzing and preprocessing of the data, it is now time when we feed the data into the KNN model. To do this, we  use sklearn's inbuilt function neighbors which has a class called KNeigborsClassifier in it. Let's start by importing the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Distance Metrics\n",
    "\n",
    "How do we measure distance?\n",
    "\n",
    "* Euclidian\n",
    "$$\n",
    "d_{euclidean} = \\sqrt{\\sum_{i=1}^{n}{(x_i - y_i)^2}}\n",
    "$$\n",
    "\n",
    "* Manhattan\n",
    "$$\n",
    "d_{manhattan} = \\sum_{i=1}^{n}{|x_i - y_i|}\n",
    "$$\n",
    "\n",
    "* Minkowski (default)\n",
    "$$\n",
    "d_{minkowski} = (\\sum_{i=1}^{n}{|x_i - y_i|^p})^{\\frac{1}{p}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename =r'Im1.png', width = 400, height = 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear from the above figure that if k = 6 and X = (X1, X2, ..., Xn), our goal is to find a class label from {C1, C2, C3, C4} for the unknown data X. As we can see in Figure, of the six closest neighbors, four belong to C1, one belongs to C2, one belongs to C3, and zero belong to C4. Therefore, by a majority vote, X is assigned to C1, which is a predominant class.\n",
    "\n",
    "## An Informal kNN Algorithm\n",
    "The kNN algorithm can be summarized in the following simple steps:\n",
    "\n",
    "1) Determine k (the selection of k depends on your data and project requirements; there is no magic formula for k).<br>\n",
    "2) Calculate the distances between the new input and all the training data (as with k, the selection of a distance function also depends on the type of data).<br>\n",
    "3) Sort the distance and determine the k nearest neighbors based on the kth minimum distance.<br>\n",
    "4) Gather the categories of those neighbors.<br>\n",
    "5) Determine the category based on majority vote.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a K-Nearest Neighbor Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to decide the best value for hyperparameter k (number of neighbors), We need to train and test your model on 10 different k values and finally use the one that gives the best results.\n",
    "\n",
    "Let's initialize a variable neighbors(k) which will have values ranging from 1-9 and two numpy zero matrices namely train_accuracy and test_accuracy each for training and testing accuracy. We need them later to plot a graph to choose the best neighbor value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors = np.arange(1, 9)                  # number of neighbors\n",
    "train_accuracy = np.zeros(len(neighbors))    # Declare and initialise the matrix\n",
    "test_accuracy = np.zeros(len(neighbors))     # Declare and initialise the matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following piece of code is where all the processing will happen. We enumerate over all the nine neighbor values and for each neighbor we then predict both on training and testing data. Finally, store the accuracy in the train_accuracy and test_accuracy numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,k in enumerate(neighbors):                          # for loop that checks the model for neighbor values 1, 2, 3, ..., 9\n",
    "    knn = KNeighborsClassifier(n_neighbors = k)           # Initialise an object knn using KNeighborsClassifier method\n",
    "\n",
    "    #Fit the model\n",
    "    knn.fit(train_data, train_label)                      # Call fit method to implement the ML KNeighborsClassifier model\n",
    "\n",
    "    #Compute accuracy on the training set\n",
    "    train_accuracy[i] = knn.score(train_data, train_label)   # Save the score value in the train_accuracy array\n",
    "\n",
    "    #Compute accuracy on the test set\n",
    "    test_accuracy[i] = knn.score(test_data, test_label)      # Save the score value in the train_accuracy array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the training and testing accuracy using matplotlib, with accuracy vs. varying number of neighbors graph. Now we can choose the best k value at which our model performs the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delcare the size of the array\n",
    "plt.figure(figsize = (10, 6))\n",
    "plt.title('KNN accuracy with varying number of neighbors', fontsize = 20)\n",
    "plt.plot(neighbors, test_accuracy, label = 'Testing Accuracy')\n",
    "plt.plot(neighbors, train_accuracy, label = 'Training accuracy')\n",
    "plt.legend(prop = {'size': 20})\n",
    "plt.xlabel('Number of neighbors', fontsize = 20)\n",
    "plt.ylabel('Accuracy', fontsize = 20)\n",
    "plt.xticks(fontsize = 20)\n",
    "plt.yticks(fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear from the above graph when n_neighbors = 3, both the model performs the best. We now use n_neighbors=3 and re-run the training once again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare and initialise an object 'KNeighborsClassifier' with 3 neighbors\n",
    "knn = KNeighborsClassifier(n_neighbors = 3)\n",
    "\n",
    "# Fit the model\n",
    "knn.fit(train_data, train_label)\n",
    "\n",
    "# Compute accuracy on the training set\n",
    "train_accuracy = knn.score(train_data, train_label)\n",
    "\n",
    "# Compute accuracy on the test set\n",
    "test_accuracy = knn.score(test_data, test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating your Model\n",
    "In the last part of this tutorial, we evaluate the model on the testing data using a couple of techniques like confusion_matrix and classification_report. Let's first check the accuracy of the model on the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the test accuracy\n",
    "print(test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain the model accuracy to classify 96.66% of the testing data correctly. With a few lines of code, we were able to train an ML model that is now able to tell you the flower name by using only four features with 96.66% accuracy. \n",
    "### Confusion Matrix\n",
    "A confusion matrix is mainly used to describe the performance of ML model on the test data for which the true values or labels are known. Scikit-learn provides a function that calculates the confusion matrix for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library for confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Predict the results by calling a method 'predict()'\n",
    "prediction = knn.predict(test_data)\n",
    "\n",
    "# Display the confusion matrix\n",
    "confusion_matrix(test_label, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the library classification_report\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Display the report\n",
    "print(classification_report(test_label, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task:\n",
    "We provided a diabetes.csv data set (https://www.kaggle.com/uciml/pima-indians-diabetes-database) on Moodle. USe this dataset and classify the patients based on the feature 'Outcome'. O means no diabetes and 1 means diabetes. Apply kNN classification model and calculate R2 value for the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('diabetes.csv')\n",
    "\n",
    "# Display the first 5 rows of the dataframe.\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference:\n",
    "* Introduction to Machine Learning with Python, Andreas C. Müller and Sarah Guido, O'Reilly Media, Inc. October 2016.\n",
    "* <p>www.datacamp.com/community/tutorials/introduction-machine-learning-python</p>\n",
    "* <p>www.kaggle.com</p>\n",
    "* Data Algorithms, O'Reilly Media, Inc., July 2015, 778 pages."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
